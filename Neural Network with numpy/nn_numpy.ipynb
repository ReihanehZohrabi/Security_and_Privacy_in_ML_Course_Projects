{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJm9Z1k0cdmh"
      },
      "source": [
        "# Neural-Network with Numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDN075MYGesD"
      },
      "source": [
        "In this notebook, you are going to write and implement all the components required to create and train a two-layered neural network using NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt3FdxgNcdmm"
      },
      "source": [
        "## Imports & Seeding:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPZ4zlnxqhl5"
      },
      "source": [
        "Importing some common libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Et7OS7TGcdmn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(123)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa2v2-xbcdmo"
      },
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKWqV2Gycdmp"
      },
      "source": [
        "You'll train and evaluate your model on [Fashion MNIST](https://en.wikipedia.org/wiki/Fashion_MNIST) dataset. In this section, you'll download Fashion MNIST and split it into training and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMYZtSoLc7c-",
        "outputId": "f85234ca-d867-42d7-a9db-4e99871d72e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(70000, 784) (70000,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Using `fetch_openml`, download `Fashion-MNIST` \n",
        "# and save the training data and labels in `X` and `y` respectively.\n",
        "#############################\n",
        "# Your code goes here (5 points)\n",
        "dataset = fetch_openml(name='Fashion-MNIST')\n",
        "X = dataset['data']\n",
        "y = dataset.target\n",
        "#############################\n",
        "\n",
        "# Normalization:\n",
        "X = ((X / 255.) - .5) * 2\n",
        "\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDmxyMJ4dBk3",
        "outputId": "79752ca6-ee85-4351-fdf5-167549925637"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 784) (60000,) (10000, 784) (10000,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Using `train_test_split`, split your data into two sets. \n",
        "# Set the test_size to 10000\n",
        "\n",
        "#############################\n",
        "# Your code goes here (6 points)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=10000)\n",
        "#############################\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiGTXGXKcdmt"
      },
      "source": [
        "## Prepare training & validation sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba3nNYlDcdmt"
      },
      "source": [
        "We'll use only 3 classes from Fashion MNIST: Trouser, T-shirt, and Sneaker classes.\n",
        "\n",
        "The class labels for T-shirt, Trouser, and Sneaker are 0, 1, and 7 respectively.\n",
        "\n",
        "In this part, you'll limit the testing and training sets to only these three classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcBDZEtzcdmu",
        "outputId": "b3783d15-a6a7-47fe-fffe-4c21e52a9735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(18017, 784) (18017,)\n"
          ]
        }
      ],
      "source": [
        "# Modify `y_train` and `x_train`.\n",
        "# Only keep the 3 classes mentioned above. \n",
        "#############################\n",
        "# Your code goes here (4 points)\n",
        "indexes = (y_train == '0') | (y_train == '1') | (y_train == '7')\n",
        "x_train = x_train[indexes].to_numpy()\n",
        "y_train = y_train[indexes].to_numpy()\n",
        "#############################\n",
        "\n",
        "print(x_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX2hkRe1cdmw",
        "outputId": "055e4c14-44d2-4e81-ae2a-0eaf8c73d37c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2983, 784) (2983,)\n"
          ]
        }
      ],
      "source": [
        "# Modify `y_test` and `x_test`.\n",
        "# Only keep the 3 classes mentioned above. \n",
        "#############################\n",
        "# Your code goes here (4 points)\n",
        "indexes = (y_test == '0') | (y_test == '1') | (y_test == '7')\n",
        "x_test = x_test[indexes].to_numpy()\n",
        "y_test = y_test[indexes].to_numpy()\n",
        "#############################\n",
        "\n",
        "print(x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv6SMLUktWbv"
      },
      "source": [
        "## Linear & Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXlyJo5JteKC"
      },
      "source": [
        "In this part, you'll implement the forward and backward process for the following components:\n",
        "- Softmax Layer\n",
        "- Linear Layer\n",
        "- ReLU Layer\n",
        "- Sigmoid Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXtAD5uYA4sQ"
      },
      "source": [
        "### The `Softmax` Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tzaIVo-_Axp7"
      },
      "outputs": [],
      "source": [
        "class SoftMaxLayer(object):\n",
        "    def __init__(self):\n",
        "        self.inp = None\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Write the forward pass for softmax.\n",
        "        # Save the values required for the backward pass.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        self.inp = x\n",
        "        exp = np.exp(self.inp)\n",
        "        self.output = exp / np.sum(exp, axis = 1, keepdims = True)\n",
        "        #############################\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, up_grad):\n",
        "        # Write the backward pass for softmax.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "\n",
        "        #Used Jacobian-based approach \n",
        "        N = up_grad.shape[0]\n",
        "        dx = np.zeros_like(up_grad)\n",
        "\n",
        "        for i in range(N):\n",
        "            J = np.diag(self.output[i]) - np.outer(self.output[i], self.output[i])\n",
        "            dx[i] = np.dot(J, up_grad[i])\n",
        "        return dx\n",
        "        #############################\n",
        "\n",
        "    def step(self, optimizer):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcFoIDZjcdnB"
      },
      "source": [
        "### The `Linear` Layer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1strsTh6cdnG"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        # Initialize the layer's weights and biases\n",
        "        #############################\n",
        "        # Your code goes here (2 points)\n",
        "        #Xavier initialization\n",
        "        std = np.sqrt(2.0 / (in_dim + out_dim))\n",
        "        self.w = np.random.normal(0, std, size=(in_dim, out_dim))\n",
        "        self.b = np.zeros((1, out_dim))\n",
        "        #############################\n",
        "        self.dw = None\n",
        "        self.db = None\n",
        "        \n",
        "    def forward(self, inp):\n",
        "        # Compute linear layer's output.\n",
        "        # Save the value(s) required for the backward phase.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        self.inp = inp\n",
        "        z = np.dot(self.inp, self.w) + self.b\n",
        "        #############################\n",
        "        \n",
        "        return z\n",
        "    \n",
        "    def backward(self, up_grad):\n",
        "        # Calculate the gradient with respect to the weights \n",
        "        # and biases and save the results.\n",
        "        #############################\n",
        "        # Your code goes here (6 points)\n",
        "        self.dw = np.matmul(self.inp.T, up_grad).mean(axis=0)\n",
        "        self.db = np.mean(up_grad, axis=0, keepdims=True)\n",
        "        down_grad = np.dot(up_grad, self.w.T)\n",
        "        #############################\n",
        "        return down_grad\n",
        "\n",
        "    def step(self, optimizer):\n",
        "      # Update the layer's weights and biases\n",
        "      # Update previous_w_update and previous_b_update accordingly\n",
        "      #############################\n",
        "      # Your code goes here (5 points)\n",
        "        self.w -= optimizer.lr * self.dw\n",
        "        self.b -= optimizer.lr * self.db\n",
        "      #############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0Lfo-nhcdnG"
      },
      "source": [
        "### The `ReLU` Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tN6vcirMcdnH"
      },
      "outputs": [],
      "source": [
        "class RelU:\n",
        "    def __init__(self):\n",
        "        self.inp = None\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # Write the forward pass for ReLU.\n",
        "        # Save the value(s) required for the backward pass.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        self.mask = inp > 0\n",
        "        output = inp * self.mask\n",
        "        self.inp = inp\n",
        "        #############################\n",
        "        return output\n",
        "    \n",
        "    def backward(self, up_grad):\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        down_grad = up_grad * self.mask\n",
        "        #############################\n",
        "        return down_grad\n",
        "\n",
        "    def step(self, optimizer):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z00KoSI3cdnJ"
      },
      "source": [
        "### The `sigmoid` Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TTYYeL2lcdnJ"
      },
      "outputs": [],
      "source": [
        "class Sigmoid:\n",
        "    def forward(self, inp):\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        self.out = 1 / (1 + np.exp(-inp))\n",
        "        #############################\n",
        "        return self.out\n",
        "    \n",
        "    def backward(self, up_grad):\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        down_grad = self.out * (1 - self.out) * up_grad\n",
        "        #############################\n",
        "        return down_grad\n",
        "    \n",
        "    def step(self, optimizer):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zngleGY2cdnK"
      },
      "source": [
        "## `Loss` function :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISedT4FvcdnK"
      },
      "source": [
        "For this task we are going to use the [Cross-Entropy Loss](https://en.wikipedia.org/wiki/Cross_entropy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XQyz4ybycdnL"
      },
      "outputs": [],
      "source": [
        "class CELoss():\n",
        "    def __init__(self):\n",
        "      pass\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        \n",
        "        self.yhat = pred\n",
        "        self.y = target\n",
        "        m = self.y.shape[0]\n",
        "        # Commpute and return the loss \n",
        "        #############################\n",
        "        # Your code goes here (8 points)\n",
        "        loss = (-1/m) * np.sum(self.y * np.log(self.yhat))\n",
        "        return loss\n",
        "        #############################\n",
        "        \n",
        "    \n",
        "    def backward(self):\n",
        "        # Derivative of loss_fn with respect to a the predicted label.\n",
        "        # Use `self.y` and `self.yhat` to compute and return `grad`.\n",
        "        #############################\n",
        "        # Your code goes here (6 points)\n",
        "        m = self.y.shape[0]\n",
        "        grad = (-1/m) * (self.y / (self.yhat))\n",
        "        #############################\n",
        "        return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xovZI-70kB9I"
      },
      "source": [
        "## Optimizer\n",
        "\n",
        "In this section, you'll implement an optimizer classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "h5ADTi5tkVTS"
      },
      "outputs": [],
      "source": [
        "class GradientDescent(object):\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def get_next_update(self, x, dx):\n",
        "        # Compute the new value for 'x' and return the result\n",
        "        #############################\n",
        "        # Your code goes here (2 points)\n",
        "        return x - self.lr * dx\n",
        "        #############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxxrEEovYEFi"
      },
      "source": [
        "## The Model\n",
        "Now you'll write the base class for a multi-layer perceptron network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "t8SoZeYRcdnY"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, layers, loss_fn, optimizer):\n",
        "        self.layers = layers \n",
        "        self.losses  = [] \n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # Pass `inp` to all the layers sequentially\n",
        "        # and return the result.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        x = inp\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "        #############################\n",
        "        \n",
        "    def loss(self, pred, label):\n",
        "        loss = self.loss_fn.forward(pred, label)\n",
        "        self.losses.append(loss)\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        # Start with loss function's gradient and \n",
        "        # do the backward pass on all the layers.\n",
        "        #############################\n",
        "        # Your code goes here (5 points)\n",
        "        up_grad = self.loss_fn.backward()\n",
        "        for layer in reversed(self.layers):\n",
        "            up_grad = layer.backward(up_grad)\n",
        "        #############################\n",
        "        \n",
        "    def update(self):\n",
        "        for layer in self.layers:\n",
        "          layer.step(self.optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo0rNwYciueF"
      },
      "source": [
        "The following cell encodes training labels into a one-hot representation with 3 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nhJTulaFJ4vR"
      },
      "outputs": [],
      "source": [
        "def onehot_enc(y, num_labels):\n",
        "    ary = np.zeros((y.shape[0], num_labels))\n",
        "    map = list(set(list(y)))\n",
        "    for i, val in enumerate(y):\n",
        "        ary[i, map.index(val)] = 1\n",
        "    return ary\n",
        "\n",
        "y_train = onehot_enc(y_train, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TS6S_RUwsRkF"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs, x, y):\n",
        "    for n in range(epochs):\n",
        "      # First do the forward pass. Next, compute the loss.\n",
        "      # Then do the backward pass and finally, update the parameters.\n",
        "      #############################\n",
        "      # Your code goes here (4 points)\n",
        "      pred = model.forward(x)\n",
        "      loss = model.loss(pred,y)\n",
        "      model.backward()\n",
        "      model.update()\n",
        "      #############################\n",
        "      print(f\"Loss at {n}: {loss:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1lSq2jNcdnY",
        "outputId": "f0e14d76-f88c-43d8-e203-405656c8fe9c",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss at 0: 1.028\n",
            "Loss at 1: 1.026\n",
            "Loss at 2: 1.024\n",
            "Loss at 3: 1.022\n",
            "Loss at 4: 1.020\n",
            "Loss at 5: 1.019\n",
            "Loss at 6: 1.017\n",
            "Loss at 7: 1.015\n",
            "Loss at 8: 1.013\n",
            "Loss at 9: 1.012\n",
            "Loss at 10: 1.010\n",
            "Loss at 11: 1.008\n",
            "Loss at 12: 1.007\n",
            "Loss at 13: 1.005\n",
            "Loss at 14: 1.003\n",
            "Loss at 15: 1.002\n",
            "Loss at 16: 1.000\n",
            "Loss at 17: 0.998\n",
            "Loss at 18: 0.997\n",
            "Loss at 19: 0.995\n",
            "Loss at 20: 0.993\n",
            "Loss at 21: 0.992\n",
            "Loss at 22: 0.990\n",
            "Loss at 23: 0.989\n",
            "Loss at 24: 0.987\n",
            "Loss at 25: 0.986\n",
            "Loss at 26: 0.984\n",
            "Loss at 27: 0.983\n",
            "Loss at 28: 0.981\n",
            "Loss at 29: 0.979\n",
            "Loss at 30: 0.978\n",
            "Loss at 31: 0.976\n",
            "Loss at 32: 0.975\n",
            "Loss at 33: 0.974\n",
            "Loss at 34: 0.972\n",
            "Loss at 35: 0.971\n",
            "Loss at 36: 0.969\n",
            "Loss at 37: 0.968\n",
            "Loss at 38: 0.966\n",
            "Loss at 39: 0.965\n",
            "Loss at 40: 0.964\n",
            "Loss at 41: 0.962\n",
            "Loss at 42: 0.961\n",
            "Loss at 43: 0.959\n",
            "Loss at 44: 0.958\n",
            "Loss at 45: 0.957\n",
            "Loss at 46: 0.955\n",
            "Loss at 47: 0.954\n",
            "Loss at 48: 0.953\n",
            "Loss at 49: 0.951\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the `MLP` with the following structure:\n",
        "#     Linear with 50 units --> ReLU --> Linear with 50 units --> ReLU --> Linear with 3 units --> Sigmoid --> Softmax\n",
        "# Use GradientDescent as the optimizer, set the learning rate to 0.001, and use CELoss as the loss function.\n",
        "#############################\n",
        "# Your code goes here (4 points)\n",
        "layers = [ Linear(784,50)\n",
        "          ,RelU(),\n",
        "          Linear(50,50),\n",
        "          RelU(),\n",
        "          Linear(50,3),\n",
        "          Sigmoid(),\n",
        "          SoftMaxLayer()]\n",
        "loss = CELoss()\n",
        "optimizer=GradientDescent(0.001)\n",
        "nn = MLP(layers,loss,optimizer)\n",
        "#############################\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "# Train the network using only `x_train` and `y_train` (no validation)\n",
        "train(nn, epochs, x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJec2xRJmY37"
      },
      "source": [
        "Let's plot the loss value for each iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ymaQNn70cdnZ",
        "outputId": "315da5da-99ac-4ca4-b4e6-aaa1811fc14a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(nn.losses)\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz3yqRa1cdna"
      },
      "source": [
        "**Let's also check our model's performance using the `accuracy` metric on the `testing` dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vsrIx7ii1ygE"
      },
      "outputs": [],
      "source": [
        "y_test = onehot_enc(y_test, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRqwXho7cdnd",
        "outputId": "30e7cb2a-ce1b-47cf-e189-5411a017b739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8209855849815622\n"
          ]
        }
      ],
      "source": [
        "# Compute the accuracy on the testing set\n",
        "#############################\n",
        "# # Your code goes here (7 points)\n",
        "correct = 0\n",
        "total = 0\n",
        "pred=nn.forward(x_test)\n",
        "for i in range(len(y_test)):\n",
        "    labels = np.argmax(y_test[i])\n",
        "    predicted = np.argmax(pred[i])\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    total +=1\n",
        "acc = correct/total\n",
        "#############################\n",
        "\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lYJskOvpw4TL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
