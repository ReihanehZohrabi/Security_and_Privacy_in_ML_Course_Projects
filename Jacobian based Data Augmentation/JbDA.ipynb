{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UgIHjec_9wuO"
      },
      "source": [
        "**SPML**\n",
        "\n",
        "**HW6**\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "d2QKhc9H94VM"
      },
      "source": [
        "1. Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BJAHH85K90lQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.utils\n",
        "from torchvision import models\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tLh3MCsi-LkW"
      },
      "source": [
        "2. Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "rB5ycNUO-RQG"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Conv2d(1,16,5), # 16*24*24\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16,32,5), # 32*20*20\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2), # 32*10*10\n",
        "            nn.Conv2d(32,64,5), # 64*6*6\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2) #64*3*3\n",
        "        )\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(64*3*3,100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100,10)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.layer(x)\n",
        "        out = out.view(-1,64*3*3)\n",
        "        out = self.fc_layer(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xlgMDm1R_bzk"
      },
      "source": [
        "3. Load original dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "563DKviw_UYO",
        "outputId": "84d54862-fbd5-4834-bc38-734b73cd83f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 92682315.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 33398316.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 28763122.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 19031497.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# todo: Define Batch size & Load MNIST dataset #\n",
        "\n",
        "\n",
        "device = torch.device(torch.cuda.current_device()) if torch.cuda.is_available() else torch.device('cpu')\n",
        "np.random.seed(0)\n",
        "\n",
        "batch_size = 128\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "mnist_train = dsets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "mnist_test = dsets.MNIST(root='./data', train=False, transform=transform)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A5ElW2ZxYErl"
      },
      "source": [
        "## Training the substitute model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX0w-T0waraC",
        "outputId": "8fe3f0e1-4531-4b28-827e-660f19a29170"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oracle = CNN().cuda()\n",
        "\n",
        "# todo: load oracle's checkpoint\n",
        "\n",
        "oracle_checkpoint_path = '/content/drive/MyDrive/MSC1401_1/SPML/HW6/checkpoint.pth'\n",
        "oracle.load_state_dict(torch.load(oracle_checkpoint_path))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AcFGZQ1EYErl"
      },
      "source": [
        "Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "collapsed": true,
        "id": "D0oRvs5wYErl"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        ### TO DO ###\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 5),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(32 * 10 * 10, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 10)\n",
        "        )\n",
        "        #############\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### TO DO ###\n",
        "        out = self.conv_layer(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc_layer(out)\n",
        "        return out\n",
        "        #############\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ2urUdbYErl"
      },
      "source": [
        "Implement Jacobian-based Data Augmentation Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "collapsed": true,
        "id": "lRNWEXhuYErl"
      },
      "outputs": [],
      "source": [
        "def train_JDA(net, X, oracle, epochs=45, lmbd=0.1, max_rho=6):\n",
        "    ### TO DO ###\n",
        "    ### Note that you can only use the prediction labels in the training\n",
        "    ### You can get the prediction labels using net(X).max(1).indices\n",
        "    for rho in range(max_rho):\n",
        "        D = []\n",
        "        for x in X:\n",
        "            x = x.cuda()\n",
        "            pred_labels = oracle(x).max(1).indices.item()\n",
        "            D.append((x, pred_labels))\n",
        "\n",
        "        # Train F on D to evaluate parameters θF\n",
        "        optimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for x, labels in D:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = net(x.unsqueeze(0))\n",
        "                loss = criterion(outputs, torch.tensor([labels]).cuda())\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(D):.4f}\")\n",
        "\n",
        "        # Perform Jacobian-based dataset augmentation\n",
        "        augmented_X = []\n",
        "        with torch.no_grad():\n",
        "            for x, labels in D:\n",
        "                outputs = net(x.unsqueeze(0))\n",
        "                jacobian = torch.autograd.functional.jacobian(lambda x: net(x), x.unsqueeze(0))\n",
        "                augmented_x = x + lmbd * torch.sign(jacobian[0][labels])\n",
        "                augmented_x = augmented_x.squeeze(0)\n",
        "\n",
        "                augmented_X.append(augmented_x)\n",
        "\n",
        "        X = X + augmented_X\n",
        "\n",
        "\n",
        "\n",
        "    return net\n",
        "    #############\n",
        "\n",
        "def compute_accuracy(net, X, y):\n",
        "    ### TO DO ###\n",
        "    net.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs = X.cuda()\n",
        "        labels = y.cuda()\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100.0 * correct / total\n",
        "    return accuracy\n",
        "    #############"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "collapsed": true,
        "id": "JfodgFU9YErm"
      },
      "outputs": [],
      "source": [
        "### TO DO ###\n",
        "### Select 100 images with their labels from the dataset, randomly\n",
        "indices = np.random.choice(len(mnist_train), size=100, replace=False)\n",
        "JDA_train_X = [mnist_train[i][0] for i in indices]\n",
        "JDA_train_y = [mnist_train[i][1] for i in indices]\n",
        "#############\n",
        "net = Net().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7HQNXfYJYErm",
        "outputId": "2d20b19c-e5f8-46d5-ec68-0d7babe2bda3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/45], Loss: 2.2400\n",
            "Epoch [2/45], Loss: 2.1754\n",
            "Epoch [3/45], Loss: 2.0856\n",
            "Epoch [4/45], Loss: 1.9336\n",
            "Epoch [5/45], Loss: 1.7287\n",
            "Epoch [6/45], Loss: 1.5984\n",
            "Epoch [7/45], Loss: 1.5262\n",
            "Epoch [8/45], Loss: 1.4683\n",
            "Epoch [9/45], Loss: 1.4134\n",
            "Epoch [10/45], Loss: 1.3591\n",
            "Epoch [11/45], Loss: 1.3008\n",
            "Epoch [12/45], Loss: 1.2350\n",
            "Epoch [13/45], Loss: 1.1601\n",
            "Epoch [14/45], Loss: 1.0805\n",
            "Epoch [15/45], Loss: 1.0002\n",
            "Epoch [16/45], Loss: 0.9241\n",
            "Epoch [17/45], Loss: 0.8549\n",
            "Epoch [18/45], Loss: 0.7952\n",
            "Epoch [19/45], Loss: 0.7423\n",
            "Epoch [20/45], Loss: 0.6968\n",
            "Epoch [21/45], Loss: 0.6571\n",
            "Epoch [22/45], Loss: 0.6221\n",
            "Epoch [23/45], Loss: 0.5904\n",
            "Epoch [24/45], Loss: 0.5606\n",
            "Epoch [25/45], Loss: 0.5337\n",
            "Epoch [26/45], Loss: 0.5060\n",
            "Epoch [27/45], Loss: 0.4799\n",
            "Epoch [28/45], Loss: 0.4562\n",
            "Epoch [29/45], Loss: 0.4310\n",
            "Epoch [30/45], Loss: 0.4069\n",
            "Epoch [31/45], Loss: 0.3841\n",
            "Epoch [32/45], Loss: 0.3622\n",
            "Epoch [33/45], Loss: 0.3411\n",
            "Epoch [34/45], Loss: 0.3213\n",
            "Epoch [35/45], Loss: 0.3033\n",
            "Epoch [36/45], Loss: 0.2850\n",
            "Epoch [37/45], Loss: 0.2688\n",
            "Epoch [38/45], Loss: 0.2533\n",
            "Epoch [39/45], Loss: 0.2369\n",
            "Epoch [40/45], Loss: 0.2227\n",
            "Epoch [41/45], Loss: 0.2090\n",
            "Epoch [42/45], Loss: 0.1972\n",
            "Epoch [43/45], Loss: 0.1862\n",
            "Epoch [44/45], Loss: 0.1721\n",
            "Epoch [45/45], Loss: 0.1625\n",
            "Epoch [1/45], Loss: 0.2110\n",
            "Epoch [2/45], Loss: 0.1946\n",
            "Epoch [3/45], Loss: 0.1766\n",
            "Epoch [4/45], Loss: 0.1644\n",
            "Epoch [5/45], Loss: 0.1553\n",
            "Epoch [6/45], Loss: 0.1480\n",
            "Epoch [7/45], Loss: 0.1416\n",
            "Epoch [8/45], Loss: 0.1352\n",
            "Epoch [9/45], Loss: 0.1305\n",
            "Epoch [10/45], Loss: 0.1258\n",
            "Epoch [11/45], Loss: 0.1213\n",
            "Epoch [12/45], Loss: 0.1174\n",
            "Epoch [13/45], Loss: 0.1138\n",
            "Epoch [14/45], Loss: 0.1110\n",
            "Epoch [15/45], Loss: 0.1084\n",
            "Epoch [16/45], Loss: 0.1064\n",
            "Epoch [17/45], Loss: 0.1044\n",
            "Epoch [18/45], Loss: 0.1022\n",
            "Epoch [19/45], Loss: 0.1012\n",
            "Epoch [20/45], Loss: 0.0993\n",
            "Epoch [21/45], Loss: 0.0975\n",
            "Epoch [22/45], Loss: 0.0967\n",
            "Epoch [23/45], Loss: 0.0956\n",
            "Epoch [24/45], Loss: 0.0941\n",
            "Epoch [25/45], Loss: 0.0922\n",
            "Epoch [26/45], Loss: 0.0919\n",
            "Epoch [27/45], Loss: 0.0910\n",
            "Epoch [28/45], Loss: 0.0892\n",
            "Epoch [29/45], Loss: 0.0882\n",
            "Epoch [30/45], Loss: 0.0866\n",
            "Epoch [31/45], Loss: 0.0851\n",
            "Epoch [32/45], Loss: 0.0851\n",
            "Epoch [33/45], Loss: 0.0838\n",
            "Epoch [34/45], Loss: 0.0819\n",
            "Epoch [35/45], Loss: 0.0813\n",
            "Epoch [36/45], Loss: 0.0793\n",
            "Epoch [37/45], Loss: 0.0795\n",
            "Epoch [38/45], Loss: 0.0775\n",
            "Epoch [39/45], Loss: 0.0764\n",
            "Epoch [40/45], Loss: 0.0763\n",
            "Epoch [41/45], Loss: 0.0740\n",
            "Epoch [42/45], Loss: 0.0752\n",
            "Epoch [43/45], Loss: 0.0732\n",
            "Epoch [44/45], Loss: 0.0723\n",
            "Epoch [45/45], Loss: 0.0709\n",
            "Epoch [1/45], Loss: 0.0447\n",
            "Epoch [2/45], Loss: 0.0426\n",
            "Epoch [3/45], Loss: 0.0394\n",
            "Epoch [4/45], Loss: 0.0385\n",
            "Epoch [5/45], Loss: 0.0375\n",
            "Epoch [6/45], Loss: 0.0364\n",
            "Epoch [7/45], Loss: 0.0352\n",
            "Epoch [8/45], Loss: 0.0346\n",
            "Epoch [9/45], Loss: 0.0332\n",
            "Epoch [10/45], Loss: 0.0325\n",
            "Epoch [11/45], Loss: 0.0319\n",
            "Epoch [12/45], Loss: 0.0314\n",
            "Epoch [13/45], Loss: 0.0303\n",
            "Epoch [14/45], Loss: 0.0290\n",
            "Epoch [15/45], Loss: 0.0283\n",
            "Epoch [16/45], Loss: 0.0283\n",
            "Epoch [17/45], Loss: 0.0268\n",
            "Epoch [18/45], Loss: 0.0269\n",
            "Epoch [19/45], Loss: 0.0258\n",
            "Epoch [20/45], Loss: 0.0242\n",
            "Epoch [21/45], Loss: 0.0248\n",
            "Epoch [22/45], Loss: 0.0237\n",
            "Epoch [23/45], Loss: 0.0233\n",
            "Epoch [24/45], Loss: 0.0226\n",
            "Epoch [25/45], Loss: 0.0213\n",
            "Epoch [26/45], Loss: 0.0215\n",
            "Epoch [27/45], Loss: 0.0207\n",
            "Epoch [28/45], Loss: 0.0202\n",
            "Epoch [29/45], Loss: 0.0196\n",
            "Epoch [30/45], Loss: 0.0191\n",
            "Epoch [31/45], Loss: 0.0190\n",
            "Epoch [32/45], Loss: 0.0179\n",
            "Epoch [33/45], Loss: 0.0177\n",
            "Epoch [34/45], Loss: 0.0167\n",
            "Epoch [35/45], Loss: 0.0172\n",
            "Epoch [36/45], Loss: 0.0164\n",
            "Epoch [37/45], Loss: 0.0162\n",
            "Epoch [38/45], Loss: 0.0152\n",
            "Epoch [39/45], Loss: 0.0151\n",
            "Epoch [40/45], Loss: 0.0146\n",
            "Epoch [41/45], Loss: 0.0149\n",
            "Epoch [42/45], Loss: 0.0136\n",
            "Epoch [43/45], Loss: 0.0140\n",
            "Epoch [44/45], Loss: 0.0130\n",
            "Epoch [45/45], Loss: 0.0132\n",
            "Epoch [1/45], Loss: 0.0266\n",
            "Epoch [2/45], Loss: 0.0198\n",
            "Epoch [3/45], Loss: 0.0211\n",
            "Epoch [4/45], Loss: 0.0172\n",
            "Epoch [5/45], Loss: 0.0190\n",
            "Epoch [6/45], Loss: 0.0182\n",
            "Epoch [7/45], Loss: 0.0182\n",
            "Epoch [8/45], Loss: 0.0177\n",
            "Epoch [9/45], Loss: 0.0167\n",
            "Epoch [10/45], Loss: 0.0161\n",
            "Epoch [11/45], Loss: 0.0159\n",
            "Epoch [12/45], Loss: 0.0154\n",
            "Epoch [13/45], Loss: 0.0151\n",
            "Epoch [14/45], Loss: 0.0154\n",
            "Epoch [15/45], Loss: 0.0154\n",
            "Epoch [16/45], Loss: 0.0157\n",
            "Epoch [17/45], Loss: 0.0162\n",
            "Epoch [18/45], Loss: 0.0162\n",
            "Epoch [19/45], Loss: 0.0163\n",
            "Epoch [20/45], Loss: 0.0163\n",
            "Epoch [21/45], Loss: 0.0160\n",
            "Epoch [22/45], Loss: 0.0158\n",
            "Epoch [23/45], Loss: 0.0154\n",
            "Epoch [24/45], Loss: 0.0155\n",
            "Epoch [25/45], Loss: 0.0150\n",
            "Epoch [26/45], Loss: 0.0147\n",
            "Epoch [27/45], Loss: 0.0143\n",
            "Epoch [28/45], Loss: 0.0141\n",
            "Epoch [29/45], Loss: 0.0138\n",
            "Epoch [30/45], Loss: 0.0135\n",
            "Epoch [31/45], Loss: 0.0134\n",
            "Epoch [32/45], Loss: 0.0131\n",
            "Epoch [33/45], Loss: 0.0129\n",
            "Epoch [34/45], Loss: 0.0127\n",
            "Epoch [35/45], Loss: 0.0125\n",
            "Epoch [36/45], Loss: 0.0123\n",
            "Epoch [37/45], Loss: 0.0121\n",
            "Epoch [38/45], Loss: 0.0120\n",
            "Epoch [39/45], Loss: 0.0118\n",
            "Epoch [40/45], Loss: 0.0116\n",
            "Epoch [41/45], Loss: 0.0114\n",
            "Epoch [42/45], Loss: 0.0114\n",
            "Epoch [43/45], Loss: 0.0111\n",
            "Epoch [44/45], Loss: 0.0109\n",
            "Epoch [45/45], Loss: 0.0108\n",
            "Epoch [1/45], Loss: 0.0358\n",
            "Epoch [2/45], Loss: 0.0369\n",
            "Epoch [3/45], Loss: 0.0322\n",
            "Epoch [4/45], Loss: 0.0307\n",
            "Epoch [5/45], Loss: 0.0282\n",
            "Epoch [6/45], Loss: 0.0275\n",
            "Epoch [7/45], Loss: 0.0266\n",
            "Epoch [8/45], Loss: 0.0255\n",
            "Epoch [9/45], Loss: 0.0247\n",
            "Epoch [10/45], Loss: 0.0243\n",
            "Epoch [11/45], Loss: 0.0238\n",
            "Epoch [12/45], Loss: 0.0234\n",
            "Epoch [13/45], Loss: 0.0229\n",
            "Epoch [14/45], Loss: 0.0227\n",
            "Epoch [15/45], Loss: 0.0218\n",
            "Epoch [16/45], Loss: 0.0216\n",
            "Epoch [17/45], Loss: 0.0216\n",
            "Epoch [18/45], Loss: 0.0214\n",
            "Epoch [19/45], Loss: 0.0210\n",
            "Epoch [20/45], Loss: 0.0208\n",
            "Epoch [21/45], Loss: 0.0206\n",
            "Epoch [22/45], Loss: 0.0203\n",
            "Epoch [23/45], Loss: 0.0200\n",
            "Epoch [24/45], Loss: 0.0195\n",
            "Epoch [25/45], Loss: 0.0201\n",
            "Epoch [26/45], Loss: 0.0199\n",
            "Epoch [27/45], Loss: 0.0196\n",
            "Epoch [28/45], Loss: 0.0190\n",
            "Epoch [29/45], Loss: 0.0191\n",
            "Epoch [30/45], Loss: 0.0188\n",
            "Epoch [31/45], Loss: 0.0184\n",
            "Epoch [32/45], Loss: 0.0182\n",
            "Epoch [33/45], Loss: 0.0183\n",
            "Epoch [34/45], Loss: 0.0181\n",
            "Epoch [35/45], Loss: 0.0177\n",
            "Epoch [36/45], Loss: 0.0178\n",
            "Epoch [37/45], Loss: 0.0178\n",
            "Epoch [38/45], Loss: 0.0175\n",
            "Epoch [39/45], Loss: 0.0175\n",
            "Epoch [40/45], Loss: 0.0175\n",
            "Epoch [41/45], Loss: 0.0171\n",
            "Epoch [42/45], Loss: 0.0171\n",
            "Epoch [43/45], Loss: 0.0166\n",
            "Epoch [44/45], Loss: 0.0166\n",
            "Epoch [45/45], Loss: 0.0164\n",
            "Epoch [1/45], Loss: 0.0244\n",
            "Epoch [2/45], Loss: 0.0226\n",
            "Epoch [3/45], Loss: 0.0218\n",
            "Epoch [4/45], Loss: 0.0215\n",
            "Epoch [5/45], Loss: 0.0212\n",
            "Epoch [6/45], Loss: 0.0209\n",
            "Epoch [7/45], Loss: 0.0208\n",
            "Epoch [8/45], Loss: 0.0203\n",
            "Epoch [9/45], Loss: 0.0204\n",
            "Epoch [10/45], Loss: 0.0202\n",
            "Epoch [11/45], Loss: 0.0202\n",
            "Epoch [12/45], Loss: 0.0200\n",
            "Epoch [13/45], Loss: 0.0199\n",
            "Epoch [14/45], Loss: 0.0194\n",
            "Epoch [15/45], Loss: 0.0193\n",
            "Epoch [16/45], Loss: 0.0193\n",
            "Epoch [17/45], Loss: 0.0194\n",
            "Epoch [18/45], Loss: 0.0188\n",
            "Epoch [19/45], Loss: 0.0189\n",
            "Epoch [20/45], Loss: 0.0187\n",
            "Epoch [21/45], Loss: 0.0184\n",
            "Epoch [22/45], Loss: 0.0184\n",
            "Epoch [23/45], Loss: 0.0184\n",
            "Epoch [24/45], Loss: 0.0178\n",
            "Epoch [25/45], Loss: 0.0181\n",
            "Epoch [26/45], Loss: 0.0180\n",
            "Epoch [27/45], Loss: 0.0177\n",
            "Epoch [28/45], Loss: 0.0175\n",
            "Epoch [29/45], Loss: 0.0177\n",
            "Epoch [30/45], Loss: 0.0176\n",
            "Epoch [31/45], Loss: 0.0171\n",
            "Epoch [32/45], Loss: 0.0174\n",
            "Epoch [33/45], Loss: 0.0172\n",
            "Epoch [34/45], Loss: 0.0173\n",
            "Epoch [35/45], Loss: 0.0174\n",
            "Epoch [36/45], Loss: 0.0171\n",
            "Epoch [37/45], Loss: 0.0167\n",
            "Epoch [38/45], Loss: 0.0168\n",
            "Epoch [39/45], Loss: 0.0168\n",
            "Epoch [40/45], Loss: 0.0166\n",
            "Epoch [41/45], Loss: 0.0172\n",
            "Epoch [42/45], Loss: 0.0162\n",
            "Epoch [43/45], Loss: 0.0164\n",
            "Epoch [44/45], Loss: 0.0165\n",
            "Epoch [45/45], Loss: 0.0162\n"
          ]
        }
      ],
      "source": [
        "net = train_JDA(net, JDA_train_X, oracle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "collapsed": true,
        "id": "lQs24iepYErm"
      },
      "outputs": [],
      "source": [
        "### TO DO ###\n",
        "### load the whole test and train dataset in one full batch\n",
        "batch_size = len(mnist_train)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
        "train_X, train_y = next(iter(train_loader))\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=len(mnist_test), shuffle=True)\n",
        "test_X, test_y = next(iter(test_loader))\n",
        "#############"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BgPy9iW2YErm",
        "outputId": "af2ec702-e720-46cf-a2a6-fb01837dfe2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy: 37.93%\n",
            "Test Accuracy: 38.70%\n"
          ]
        }
      ],
      "source": [
        "### TO DO ###\n",
        "### compute accuracy of net on train and test\n",
        "train_accuracy = compute_accuracy(net, train_X, train_y)\n",
        "test_accuracy = compute_accuracy(net, test_X, test_y)\n",
        "\n",
        "print('Train Accuracy: {:.2f}%'.format(train_accuracy))\n",
        "print('Test Accuracy: {:.2f}%'.format(test_accuracy))\n",
        "#############"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
